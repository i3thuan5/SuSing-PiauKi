[model_config_name=tw]
vocab_size=44000
target_num=81
#hyper_params:
init_scale=0.04
learning_rate=0.1
max_grad_norm=10
num_layers=2
num_steps=30
hidden_size=128
max_epoch=5
max_max_epoch=10
keep_prob=1.0
lr_decay=0.85
batch_size=1
